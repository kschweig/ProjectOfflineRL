# Environment
env='Breakout'

# timesteps to train for
max_timesteps = 10 000 000

# Exploration
start_timesteps=20 000
initial_eps=1
end_eps=1e-2
eps_decay_period=250 000

# Evaluation
eval_freq=50 000
eval_eps=1e-3
window = 5
eval_iters = 10

# Learning
discount=0.99
buffer_size=1 000 000
batch_size=32
# online algorithms does only get trained every train_freq step, therefore offline algorithms get this times more training
train_freq= 4
soft_target_update=False
target_update_freq=8 000
tau=1

# Optimizer, either 'Adam' or 'RMSProp'!
optimizer='Adam'
lr=0.0000625

eps=0.00015

# should duelling dqn be used?
duelling=False

### generating policies
# size is already defined by max_timesteps, as we want the same number of steps
# so we define the number of policies here
policies=2 500 000
# use agarwal et. al's method (observations directly obtained from dqn agent with its current eps)
# instead of fujimoto et al's method (low_noise_p with gen_eps or eval_eps)
use_train_buffer = True
# Probability of a low noise episode when generating buffer
# during low noise episode use eval_eps
low_noise_p=0.2
# Probability of taking a random action when generating buffer, during non-low noise episode
gen_eps=0.2
###

# BCQ
threshold=0.3
# REM
heads=200
# QR-DQN