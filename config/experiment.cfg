# timesteps to train for
max_timesteps = 1 000 000

# Exploration
start_timesteps=20 000
initial_eps=1
end_eps=1e-2
eps_decay_period=250 000

# Evaluation
eval_freq=50 000
eval_eps=1e-3

# Learning
discount=0.99
buffer_size=1 000 000
batch_size=32
# online algorithms does only get trained every train_freq step, therefore offline algorithms get this times more training
train_freq= 4
soft_target_update=False
target_update_freq=8 000
tau=1

# Optimizer, either 'Adam' or 'RMSProp'!
optimizer='Adam'
lr=0.0000625
eps=0.00015

# should duelling dqn be used?
duelling=False

### generating policies
# Probability of a low noise episode when generating buffer
low_noise_p=0.2
# Probability of taking a random action when generating buffer, during non-low noise episode
rand_action_p=0.2
###

# BCQ
threshold=0.3
# REM
heads=200
# QR-DQN